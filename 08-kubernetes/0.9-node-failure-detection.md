# How Kubernetes Detects Node Failure

This document explains **how Kubernetes detects node failure**, step by step, including **components involved, timing, decision logic, and what happens to Pods**. A full **flow diagram (text-based)** and **real example** are included.

---

## Key Components Involved

### 1. kubelet (on each node)

* Runs on every worker node
* Sends **NodeStatus** updates to the API Server
* Sends **Lease heartbeats** via `kube-node-lease`

### 2. API Server

* Receives heartbeats and NodeStatus updates
* Stores state in etcd

### 3. etcd

* Persistent store for Node objects and Lease objects

### 4. Node Controller (inside kube-controller-manager)

* Watches Node objects
* Detects missing heartbeats
* Marks nodes unhealthy
* Triggers Pod eviction

### 5. Scheduler

* Reschedules Pods from failed nodes onto healthy nodes

---

## Two Heartbeat Mechanisms

### A. NodeStatus Updates (kubelet → API Server)

* Interval: **~10s** (default)
* Contains:

  * Conditions (Ready, MemoryPressure, DiskPressure)
  * Capacity & allocatable resources

### B. Lease Heartbeats (Lightweight)

* Stored in: `kube-node-lease` namespace
* Interval: **~10s**
* Used for **fast failure detection**
* Preferred in modern Kubernetes

---

## Failure Detection Timers (Defaults)

| Setting                     | Default   |
| --------------------------- | --------- |
| NodeStatus update frequency | 10s       |
| NodeMonitorGracePeriod      | 40s       |
| Pod eviction timeout        | 5 minutes |

---

## Full Failure Detection Flow (Step by Step)

```
[ Node Running Normally ]
        |
        | kubelet sends NodeStatus + Lease (every 10s)
        v
[ API Server ]
        |
        | stores updates
        v
[ etcd ]

--- FAILURE OCCURS ---

[ Node Crashes / Network Lost ]
        |
        | kubelet stops sending heartbeats
        v
[ No Lease Update for > 40s ]
        |
        v
[ Node Controller detects missing heartbeat ]
        |
        | sets NodeReady = Unknown
        v
[ Node marked NotReady ]
        |
        | adds taint:
        | node.kubernetes.io/not-ready:NoExecute
        v
[ Pods become candidates for eviction ]
        |
        | after eviction timeout
        v
[ Pods deleted from failed node ]
        |
        v
[ Scheduler reschedules Pods ]
        |
        v
[ Pods running on healthy nodes ]
```

---

## Node Condition Transition

```
Ready → Unknown → NotReady
```

* **Unknown**: kubelet unreachable
* **NotReady**: confirmed unhealthy

---

## What Exactly Triggers Node Failure Detection?

Node Controller checks:

* Last successful **Lease renewal**
* Last **NodeStatus update timestamp**

If **no update within NodeMonitorGracePeriod**:

* Node is considered unhealthy

---

## What Happens to Pods on a Failed Node?

### Case 1: Pods without tolerations

* Evicted immediately after NotReady

### Case 2: Pods with tolerations

Example:

```yaml
tolerations:
- key: "node.kubernetes.io/not-ready"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 300
```

* Pod stays for 300s before eviction

### Case 3: DaemonSet Pods

* **Not evicted** automatically
* Recreated when node comes back

---

## Real Example Scenario

### Cluster Setup

* 3 nodes: node-1, node-2, node-3
* App deployed with 3 replicas

### Failure Event

* node-2 crashes (power/network failure)

### Timeline

```
T0  : node-2 stops responding
T+40s : NodeController marks node-2 NotReady
T+40s : Taint applied (NoExecute)
T+5m  : Pods evicted from node-2
T+5m+ : Scheduler places Pods on node-1 & node-3
```

### Result

* Application remains available
* Replica count restored

---

## If the Node Comes Back

* kubelet resumes heartbeats
* Node condition becomes Ready
* Taints removed automatically
* New Pods can be scheduled

---

## Important Notes

* Kubernetes **does NOT reboot nodes**
* Kubernetes **does NOT migrate running containers**
* Pods are always **recreated**, not moved
* etcd only stores state, not health logic

---

## One-Line Summary

> Kubernetes detects node failure by **missing kubelet heartbeats**, marks the node **NotReady**, taints it, evicts Pods, and **reschedules them on healthy nodes**.
