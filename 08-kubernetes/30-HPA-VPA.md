# âš–ï¸ HPA & VPA Demo in Kubernetes

This demo explains **Horizontal Pod Autoscaler (HPA)** and **Vertical Pod Autoscaler (VPA)** with:

* real-world use cases ðŸ§ 
* step-by-step setup ðŸªœ
* working YAML examples ðŸ“„

---

# ðŸš€ Part 1: Horizontal Pod Autoscaler (HPA)

## ðŸ§  What is HPA?

HPA automatically **scales the number of Pods** based on resource usage (CPU / memory) or custom metrics.

> ðŸ“ˆ More traffic â†’ more Pods

---

## ðŸŽ¯ HPA Use Case (Real World)

ðŸ‘‰ A web application where traffic spikes during peak hours.

* Low traffic â†’ 1â€“2 Pods ðŸŒ™
* High traffic â†’ 10+ Pods ðŸŒž

---

## âš™ï¸ Prerequisites for HPA

âœ… Metrics Server must be running

```bash
kubectl get deployment metrics-server -n kube-system
```

If not installed:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

---

## ðŸ§© Step 1: Create Deployment for HPA

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hpa-demo
  template:
    metadata:
      labels:
        app: hpa-demo
    spec:
      containers:
      - name: app
        image: k8s.gcr.io/hpa-example
        resources:
          requests:
            cpu: "100m"
          limits:
            cpu: "500m"
```

---

## ðŸ§© Step 2: Create HPA Object

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-demo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpa-demo
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

---

## ðŸ§ª Step 3: Generate Load

```bash
kubectl run -i --tty load-generator --image=busybox /bin/sh
```

Inside the pod:

```bash
while true; do wget -q -O- http://hpa-demo; done
```

---

## ðŸ“Š Step 4: Observe HPA Behavior

```bash
kubectl get hpa -w
kubectl get pods
```

ðŸ” Result:

* CPU â†‘ â†’ Pods scale out âž•
* CPU â†“ â†’ Pods scale in âž–

---

## ðŸ”„ HPA Internal Flow

1ï¸âƒ£ Metrics Server collects CPU data ðŸ“Š
2ï¸âƒ£ HPA Controller evaluates metrics ðŸ§ 
3ï¸âƒ£ Desired replicas calculated âž—
4ï¸âƒ£ Deployment updated ðŸ“„
5ï¸âƒ£ New Pods created or removed âš™ï¸

---

# ðŸ§  Part 2: Vertical Pod Autoscaler (VPA)

## ðŸ§  What is VPA?

VPA automatically **adjusts CPU & memory requests/limits** of Pods.

> ðŸ“¦ Same Pods, better sizing

---

## ðŸŽ¯ VPA Use Case (Real World)

ðŸ‘‰ Backend service with unpredictable memory usage.

* Over-allocated resources â†’ wasted money ðŸ’¸
* Under-allocated resources â†’ OOMKills ðŸ’¥

VPA solves this by right-sizing Pods ðŸŽ¯

---

## âš™ï¸ Prerequisites for VPA

VPA is **not installed by default**.

```bash
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh
```

Verify:

```bash
kubectl get pods -n kube-system | grep vpa
```

---

## ðŸ§© Step 1: Create Deployment for VPA

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vpa-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vpa-demo
  template:
    metadata:
      labels:
        app: vpa-demo
    spec:
      containers:
      - name: app
        image: nginx
        resources:
          requests:
            cpu: "50m"
            memory: "50Mi"
```

---

## ðŸ§© Step 2: Create VPA Object

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-demo
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vpa-demo
  updatePolicy:
    updateMode: Auto
```

---

## ðŸ”„ Step 3: Observe VPA in Action

```bash
kubectl describe vpa vpa-demo
```

ðŸ” What happens:

* VPA recommends new CPU/memory values ðŸ“ˆ
* Pods are restarted ðŸ”„
* New requests applied automatically

---

## ðŸ§  VPA Internal Flow

1ï¸âƒ£ Metrics collected over time ðŸ“Š
2ï¸âƒ£ VPA Recommender calculates ideal resources ðŸ§®
3ï¸âƒ£ Admission Controller mutates Pod spec âœï¸
4ï¸âƒ£ Pod recreated with new requests ðŸ”

---

# âš ï¸ HPA vs VPA (Important Rule)

âŒ Do NOT use HPA and VPA on **CPU at the same time**

| Feature            | HPA            | VPA             |
| ------------------ | -------------- | --------------- |
| Scales Pods        | âœ…              | âŒ               |
| Changes CPU/Memory | âŒ              | âœ…               |
| Pod Restart        | âŒ              | âœ…               |
| Best For           | Traffic spikes | Resource tuning |

---

## â­ Best Practices

âœ… Use **HPA for stateless apps** ðŸŒ
âœ… Use **VPA for backend & batch workloads** ðŸ§®
âœ… Combine HPA (CPU) + VPA (Memory) carefully ðŸ§ 
âœ… Monitor before enabling Auto mode ðŸ‘€

---

## ðŸ§© One-Line Summary

> **HPA scales Pods horizontally ðŸ“ˆ, VPA scales resources vertically ðŸ“¦ â€” both solve different scaling problems in Kubernetes ðŸš€**

---

---

# ðŸ§© Why `apiVersion: apps/v1` Is Used

## â“ What does `apps/v1` mean?

In Kubernetes, every object is identified by:

```
API Group + Version
```

```yaml
apiVersion: apps/v1
kind: Deployment
```

* `apps` â†’ API group for **application workloads**
* `v1` â†’ **stable, production-ready** version

---

## ðŸ“¦ Resources That Use `apps/v1`

| Resource    | Reason                     |
| ----------- | -------------------------- |
| Deployment  | Manages ReplicaSets & Pods |
| ReplicaSet  | Ensures desired Pod count  |
| StatefulSet | Stateful workloads         |
| DaemonSet   | One Pod per Node           |

All these control Pods at scale, so they live in the **apps API group**.

---

## ðŸ—ï¸ Why Not Just `apiVersion: v1`?

`v1` (without a group) is the **core API group**, used for basic building blocks:

```yaml
kind: Pod
kind: Service
kind: ConfigMap
kind: Secret
```

Workload controllers evolved separately, so Kubernetes moved them to `apps` for:

* better separation of concerns
* safer evolution
* backward compatibility

---

## ðŸ•°ï¸ History (Interview Favorite â­)

Older, deprecated versions:

```yaml
extensions/v1beta1
apps/v1beta1
apps/v1beta2
```

ðŸš« Removed since Kubernetes **1.16**

âœ… Today, **`apps/v1` is mandatory** for Deployments.

---

## âš™ï¸ What `apps/v1` Enforces (Very Important)

In `apps/v1`, some fields are **mandatory**.

Example: `spec.selector`

```yaml
spec:
  selector:
    matchLabels:
      app: my-app
```

This prevents orphaned ReplicaSets and broken rollouts.

---

## ðŸ”„ How API Server Uses `apps/v1`

1ï¸âƒ£ YAML applied by user
2ï¸âƒ£ API Server reads `apps/v1`
3ï¸âƒ£ Routes to Apps API handler
4ï¸âƒ£ Validates against version schema
5ï¸âƒ£ Stores object in etcd
6ï¸âƒ£ Controller acts on it

---

## ðŸ§  One-Line Interview Answer

> **Deployments use `apps/v1` because they belong to the Apps API group, and `v1` is the stable, production-safe version supported by modern Kubernetes clusters.**

---

---

# ðŸ”¥ Can HPA and VPA Be Used Together?

## â“ Short Answer

âœ… **Yes â€” but only in a controlled and safe configuration**

âŒ **No â€” if both try to control the same resource (CPU)**

---

## ðŸš« Golden Rule (Very Important)

> **Never allow HPA and VPA to control the same resource metric at the same time.**

Why?

* HPA scales Pods based on resource utilization ðŸ“ˆ
* VPA changes resource requests ðŸ“¦
* Both acting on CPU causes feedback loops ðŸ”„

---

## âŒ Unsafe Combination (Do NOT Use)

```text
HPA â†’ scales Pods based on CPU utilization
VPA â†’ modifies CPU requests
```

### What Happens Internally?

1ï¸âƒ£ VPA increases CPU request ðŸ“¦
2ï¸âƒ£ CPU utilization percentage drops ðŸ“‰
3ï¸âƒ£ HPA scales Pods down âž–
4ï¸âƒ£ Load increases again ðŸ“ˆ
5ï¸âƒ£ HPA scales Pods up âž•

âš ï¸ Result: unstable scaling loop

---

## âœ… Safe & Recommended Pattern (Production Ready)

### âœ”ï¸ HPA on CPU + VPA on Memory

| Component | Controls      | Benefit                |
| --------- | ------------- | ---------------------- |
| HPA       | CPU / traffic | Handles load spikes ðŸ“ˆ |
| VPA       | Memory only   | Prevents OOMKills ðŸ’¥   |

Both autoscalers work on **different dimensions**, so no conflict occurs.

---

## ðŸ§© Safe Configuration Example

### VPA â€“ Memory Only

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vpa-demo
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  resourcePolicy:
    containerPolicies:
    - containerName: "*"
      controlledResources: ["memory"]
  updatePolicy:
    updateMode: Auto
```

### HPA â€“ CPU Based

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
```

---

## ðŸ§  Alternative Safe Patterns

### ðŸ”¹ VPA in Recommendation Mode

```yaml
updatePolicy:
  updateMode: Off
```

* No Pod restarts ðŸ”•
* Only recommendations shown ðŸ“Š
* Safe with any HPA setup

---

## ðŸŽ¯ Real-World Use Case

ðŸ‘‰ E-commerce backend service

* Traffic spikes â†’ handled by **HPA** ðŸ›’
* Memory-heavy workloads â†’ handled by **VPA** ðŸ§ 
* Result â†’ stable scaling + cost efficiency ðŸ’°

---

## ðŸ§  One-Line Interview Answer

> **HPA and VPA can be used together only if they do not act on the same resource. The recommended approach is HPA on CPU or traffic metrics and VPA on memory.**

---

ðŸŽ‰ You now have a complete **hands-on demo for HPA & VPA**, ready for labs, interviews, and production discussions!
