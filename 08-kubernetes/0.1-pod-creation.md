# Complete Story: How a Pod Is Created and Scheduled in Kubernetes

This document gives the **full end-to-end story** of:

1. **How the API Server creates a Pod**
2. **How kube-scheduler decides where to place that Pod**

With:

* Clear narrative flow
* Example **YAML** (what you write)
* Example **JSON** (what Kubernetes stores)
* Simple diagrams (text-based)

---

## Part 1: Pod Creation ‚Äî API Server Story

### üß† Core Truth

> The API Server creates a Pod by **validating a request and storing the Pod object in etcd**.

It does **not** run containers and does **not** choose nodes.

---

## Step 1: User Requests a Pod

You write a Pod manifest:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: demo-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
```

And apply it:

```bash
kubectl apply -f pod.yaml
```

---

## Step 2: kubectl ‚Üí API Server (REST Call)

`kubectl` converts YAML ‚Üí JSON and sends:

```
POST /api/v1/namespaces/default/pods
```

üìå API Server is a **REST endpoint**.

---

## Step 3: API Server Processing Pipeline

The API Server processes the request in strict order:

1. **Authentication** ‚Äì Who are you?
2. **Authorization (RBAC)** ‚Äì Can you create Pods?
3. **Admission Controllers** ‚Äì Apply policies / mutations
4. **Validation** ‚Äì Is the Pod spec valid?

If any step fails ‚Üí Pod is **not created**.

---

## Step 4: Pod Is Stored in etcd (Creation Moment)

If everything passes, API Server writes the Pod to **etcd**.

### JSON Stored in etcd (Simplified)

```json
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "demo-pod",
    "namespace": "default"
  },
  "spec": {
    "containers": [
      {
        "name": "nginx",
        "image": "nginx:latest"
      }
    ]
  },
  "status": {
    "phase": "Pending"
  }
}
```

üìå **This write = Pod creation**.

No node assigned yet.

---

## Pod State After Creation

```yaml
spec:
  nodeName: null
status:
  phase: Pending
```

The Pod now **exists**, but is waiting.

---

## Part 2: kube-scheduler Story ‚Äî Pod Placement

### üß† Core Truth

> kube-scheduler **does not create Pods**.
> It only assigns a **Node** to existing Pods.

It watches the API Server for:

* Pods with `nodeName = null`

---

## Step 5: Pod Enters Scheduler Queue

The scheduler sees:

* Pod exists
* Pod is Pending
* No node assigned

‚û°Ô∏è Pod enters the **scheduling queue**.

---

## Step 6: Filtering Phase (Hard Rules)

Scheduler checks **which nodes can run this Pod**.

### Common Filters

* CPU / Memory availability
* Node cordoned or not
* Node affinity (required)
* Taints & tolerations
* Volume attachment

---

### Example Cluster

| Node   | CPU Free  | Memory Free | Labels          |
| ------ | --------- | ----------- | --------------- |
| node-a | 2 cores   | 4Gi         | zone=us-east-1a |
| node-b | 0.5 cores | 1Gi         | zone=us-east-1b |
| node-c | 4 cores   | 8Gi         | zone=us-east-1a |

---

### Example Pod with Constraints

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: constrained-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: zone
          operator: In
          values: ["us-east-1a"]
```

---

### Filtering Result

* ‚ùå node-b ‚Üí insufficient CPU + wrong zone
* ‚úÖ node-a ‚Üí passes
* ‚úÖ node-c ‚Üí passes

Remaining nodes: **node-a, node-c**

---

## Step 7: Scoring Phase (Soft Preferences)

Scheduler now **scores** remaining nodes.

### Example Scoring (LeastAllocated)

| Node   | Free CPU | Free Memory | Score |
| ------ | -------- | ----------- | ----- |
| node-a | 2        | 4Gi         | 65    |
| node-c | 4        | 8Gi         | 95    |

‚û°Ô∏è **node-c wins**.

---

## Step 8: Binding Pod to Node

Scheduler sends a **Binding** request to API Server.

### What API Server Updates

```yaml
spec:
  nodeName: node-c
```

Pod is now **scheduled**.

---

## Step 9: kubelet Takes Over (After Scheduling)

On `node-c`:

1. kubelet sees Pod assigned to itself
2. Pulls image
3. Creates container
4. Starts the process

Pod state transitions:

```
Pending ‚Üí ContainerCreating ‚Üí Running
```

---

## End-to-End Flow Diagram

```
User (kubectl)
   |
   v
API Server
 ‚îú‚îÄ Auth / RBAC
 ‚îú‚îÄ Admission
 ‚îú‚îÄ Validation
 ‚îî‚îÄ Store Pod in etcd  ‚Üê Pod CREATED
        |
        v
     Pod Pending
        |
        v
kube-scheduler
 ‚îú‚îÄ Filter Nodes
 ‚îú‚îÄ Score Nodes
 ‚îî‚îÄ Bind Pod to Node
        |
        v
API Server updates nodeName
        |
        v
kubelet runs containers
```

---

## Key Takeaways (Interview Gold)

* API Server **creates** Pods by persisting them
* Scheduler **only assigns nodes**
* kubelet **runs containers**
* etcd is the **single source of truth**

---

If you want next:

* etcd key structure for Pods
* Scheduler logs walkthrough
* Why Pods are immutable
* Manual Pod vs Deployment lifecycle
