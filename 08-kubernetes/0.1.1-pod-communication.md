# How Pods Communicate With Each Other Across Nodes (Kubernetes)

## High‑level idea

Kubernetes provides a **flat, cluster‑wide Pod network**:

* Every Pod gets its **own IP address**
* Any Pod can reach any other Pod **directly via Pod IP**
* **No NAT** between Pods
* Communication works the **same across nodes and within a node**

This behavior is implemented by the **CNI (Container Network Interface) plugin**.

---

## Core components involved

### 1. Pod IP

* Assigned by the CNI plugin
* Unique within the cluster
* Routable from any node

### 2. Node network

* Each node has its own **node IP**
* Nodes are connected through the underlying network (VPC / LAN / data center)

### 3. CNI Plugin

Responsible for:

* Allocating Pod IPs
* Creating network interfaces inside Pods
* Programming routing / encapsulation rules

Common CNIs:

* Calico (routing or IP‑in‑IP)
* Flannel (VXLAN)
* Cilium (eBPF)
* Weave

---

## Communication flow: Pod on Node‑A → Pod on Node‑B

### Scenario

* Pod A (Node‑A): `10.244.1.5`
* Pod B (Node‑B): `10.244.2.8`

Pod A sends a packet to Pod B using Pod IP.

---

## Step‑by‑step packet journey

### Step 1: Pod sends packet

* Application in Pod A sends traffic to `10.244.2.8`
* Packet leaves Pod via its virtual interface (veth)

### Step 2: Packet reaches Node‑A network stack

* Node‑A sees destination IP belongs to another node’s Pod CIDR
* Routing rules installed by CNI decide what to do next

### Step 3: Cross‑node transport (depends on CNI)

#### Option A: Encapsulation (VXLAN / IP‑in‑IP)

* Original packet is **wrapped inside another packet**
* Outer packet destination = Node‑B IP
* Travels over the physical network

#### Option B: Native routing (no encapsulation)

* Node‑A routes packet directly to Node‑B
* Requires cloud/VPC routing support

### Step 4: Packet reaches Node‑B

* Node‑B decapsulates packet (if used)
* Destination Pod IP is local

### Step 5: Delivered to Pod‑B

* Packet sent through veth pair
* Pod‑B application receives the request

---

## ASCII Diagram: Cross‑Node Pod Communication

```
+--------------------+                 +--------------------+
|     Node A         |                 |     Node B         |
|                    |                 |                    |
|  Pod A             |                 |  Pod B             |
|  10.244.1.5        |                 |  10.244.2.8        |
|    |               |                 |               |    |
|  [veth]            |                 |            [veth]  |
|    |               |                 |               |    |
|  Node Network      |==== Encaps ==== |   Node Network     |
|  (CNI rules)       |  / Route        |   (CNI rules)      |
|                    |                 |                    |
+--------------------+                 +--------------------+
        |                                           |
        +---------- Underlying Network -------------+
                    (VPC / LAN)
```

---

## Why kube‑proxy is NOT involved here

* Pod‑to‑Pod communication **does not use Services**
* Traffic goes **directly to Pod IP**
* kube‑proxy is only used for:

  * ClusterIP
  * NodePort
  * LoadBalancer

---

## Role of routing tables

Each node maintains routes like:

```
10.244.1.0/24 → local
10.244.2.0/24 → via Node‑B
```

These routes are:

* Programmed by CNI
* Stored in kernel routing tables or eBPF maps

---

## Security controls in Pod communication

### Network Policies

* Control which Pods can talk to which Pods
* Enforced by CNI (Calico / Cilium / Weave)

### Without NetworkPolicy

* All Pods can talk to all Pods by default

---

## Key guarantees Kubernetes provides

| Guarantee           | Meaning                            |
| ------------------- | ---------------------------------- |
| Flat network        | All Pods see one network           |
| No NAT              | Source IP preserved                |
| Pod IP reachability | Pod IP reachable from any node     |
| CNI‑driven          | Implementation delegated to plugin |

---

## More Practical Examples & Deep Clarifications

---

## Example 1: Simple HTTP call between Pods on different nodes

### Setup

* Node‑A Pod: `frontend` → `10.244.1.10`
* Node‑B Pod: `backend` → `10.244.2.20`
* Backend listens on port `8080`

### What happens when frontend calls backend

```
curl http://10.244.2.20:8080
```

Flow:

1. Frontend app creates TCP packet for `10.244.2.20:8080`
2. Packet exits Pod via veth interface
3. Node‑A routing table says: `10.244.2.0/24 → Node‑B`
4. CNI encapsulates packet (VXLAN/IP‑in‑IP) OR routes it natively
5. Packet reaches Node‑B
6. Node‑B delivers packet to backend Pod veth
7. Backend app receives request

Source IP remains `10.244.1.10` (no NAT)

---

## Example 2: Same application, same behavior across nodes

Important Kubernetes guarantee:

> Applications do NOT need to know whether Pods are on the same node or different nodes.

```
Pod A (Node‑A)  ───►  Pod B (Node‑A)
Pod A (Node‑A)  ───►  Pod C (Node‑B)
```

From application perspective:

* Same Pod IP based communication
* Same latency expectations (network dependent)
* Same debugging approach

---

## Example 3: What exactly the CNI creates on the node

For every Pod, CNI creates:

* One **veth pair**

  * One end inside Pod (eth0)
  * One end on Node
* IP address assigned to Pod
* Routing entries for remote Pod CIDRs

On Node‑A:

```
ip link
veth1234 ↔ eth0 (inside Pod)
```

Routing table:

```
10.244.1.0/24 dev cni0
10.244.2.0/24 via 192.168.1.20
```

---

## Example 4: Encapsulation vs Native Routing (visual clarity)

### Encapsulation (Flannel VXLAN)

```
[Pod Packet]
SRC=10.244.1.10
DST=10.244.2.20

↓ encapsulated

[Outer Packet]
SRC=Node‑A‑IP
DST=Node‑B‑IP
```

Used when:

* Underlying network doesn’t know Pod CIDRs
* Simple cluster setup

### Native Routing (Calico in cloud)

```
Pod Packet routed directly
10.244.2.0/24 → Node‑B
```

Used when:

* Cloud/VPC supports route programming
* Better performance (no encapsulation)

---

## Example 5: NetworkPolicy impact

### Without NetworkPolicy

```
Any Pod → Any Pod  (Allowed)
```

### With NetworkPolicy

```
frontend → backend  (Allowed)
frontend → database (Denied)
```

Important:

* Packets still travel the same path
* CNI drops packets based on policy
* Kubernetes itself does not enforce policies

---

## Example 6: Common confusion points clarified

### ❌ Does kube‑proxy handle Pod‑to‑Pod traffic?

No. kube‑proxy only works with Services.

### ❌ Is NAT used between Pods?

No. Pod IPs are preserved end‑to‑end.

### ❌ Is Service required for Pod communication?

No. Service is for abstraction and load‑balancing.

### ❌ Does etcd store Pod networking state?

No. etcd stores desired state, not packet flow or routes.

---

## Example 7: Debugging cross‑node Pod communication

From Pod:

```
ip addr
ip route
ping <other‑pod‑ip>
```

From Node:

```
ip route
iptables -L
ip link
```

If ping fails:

* Check CNI Pod status
* Check node‑to‑node connectivity
* Check NetworkPolicy

---

## Mental model to remember

Think of Kubernetes Pod networking as:

> One giant flat L3 network, where CNI acts as the distributed router connecting all nodes.

---

## One‑line summary

Pods communicate across nodes by sending traffic directly to Pod IPs, while the CNI plugin transparently routes or encapsulates packets between nodes, making the entire cluster behave like a single logical network.
